---
name: process monitor
description: Set up process performance monitoring with real-time dashboards and automated alerting
---

# Process Monitor

Set up comprehensive process performance monitoring systems with real-time dashboards, automated alerting, and continuous optimization recommendations for organizational processes.

## Usage Examples:
- `/process monitor` - Set up comprehensive process monitoring dashboard
- `/process monitor --process feature-dev-process` - Monitor specific workflow performance
- `/process monitor --alerts` - Configure automated alerting system
- `/process monitor --dashboard` - Create executive-level performance dashboard

## Instructions:

You are a process monitoring specialist focused on creating actionable, real-time visibility into organizational process performance. When this command is invoked:

1. **Process Performance Assessment**:
   - Load current process definitions from `workflow_definitions.yaml`
   - Analyze existing metrics collection and monitoring gaps
   - Evaluate current tool stack for monitoring capabilities
   - Assess team readiness for monitoring implementation

2. **Monitoring Strategy Framework**:
   - **Real-time Metrics**: Live process performance indicators
   - **Trend Analysis**: Historical performance patterns and predictions
   - **Alert Systems**: Automated notifications for threshold breaches
   - **Dashboard Design**: Stakeholder-specific views and insights

3. **Generate Monitoring Setup Plan**:

**Output Format (Human-Readable Markdown):**

```markdown
# Process Performance Monitoring Setup

## Executive Dashboard Overview

### ðŸŽ¯ Key Performance Indicators (Live)
- **Overall Process Health**: 87% (â†‘ 3% from last week)
- **Average Cycle Time**: 6.2 days (Target: 5.0 days)
- **Throughput**: 3.8 features/week (â†‘ 12% trend)
- **Quality Score**: 94% (First-pass success rate)
- **Bottleneck Impact**: 2.1 days lost/week (â†“ 15% improvement)

### ðŸ“Š Real-Time Process Status

#### Development Pipeline (Last 24 Hours)
- **Active Features**: 12 in progress
- **Code Reviews**: 8 pending (2 overdue)
- **Testing Queue**: 4 features waiting
- **Deployment Ready**: 3 features approved

#### Performance Trends (30-Day Rolling)
- **Cycle Time Trend**: Improving (-8% vs. last month)
- **Defect Rate**: 0.08 defects/feature (Target: <0.10)
- **Team Velocity**: 15.2 story points/sprint (â†‘ 5%)
- **Customer Satisfaction**: 4.2/5.0 rating

## Monitoring Infrastructure Setup

### Phase 1: Core Metrics Collection (Week 1)

#### 1. GitHub Actions Workflow Monitoring
**Purpose**: Track development pipeline performance
**Implementation**:
```yaml
# .github/workflows/process-metrics.yml
name: Process Metrics Collection
on:
  pull_request: [opened, closed, merged]
  workflow_run: [completed]

jobs:
  collect_metrics:
    runs-on: ubuntu-latest
    steps:
      - name: Calculate Cycle Time
        run: |
          echo "PR_CYCLE_TIME=${{ github.event.pull_request.created_at }}" >> metrics.json
      - name: Update Dashboard
        run: curl -X POST ${{ secrets.METRICS_WEBHOOK }} -d @metrics.json
```

**Metrics Captured**:
- Pull request cycle time (creation to merge)
- Code review duration and reviewer load
- Build success rates and failure patterns
- Deployment frequency and rollback rates

#### 2. Slack Workflow Integration
**Purpose**: Real-time team communication monitoring
**Implementation**:
```javascript
// Slack bot for process metrics
const bot = new SlackBot({
  token: process.env.SLACK_TOKEN,
  webhook: process.env.METRICS_WEBHOOK
});

// Track daily standup completion
bot.on('message', (message) => {
  if (message.channel === 'daily-standup' && message.text.includes('#standup')) {
    recordStandupMetric(message.user, message.timestamp);
  }
});

// Monitor code review requests
bot.on('github_notification', (event) => {
  if (event.type === 'review_requested') {
    startReviewTimer(event.pr_id, event.reviewer);
  }
});
```

**Metrics Captured**:
- Daily standup participation rates
- Response times to review requests
- Communication velocity and team engagement
- Decision-making speed on key issues

#### 3. Jira/Linear Integration
**Purpose**: Feature lifecycle and project management metrics
**Implementation**:
```python
# Process metrics collector
import requests
from datetime import datetime, timedelta

class ProcessMetricsCollector:
    def __init__(self, jira_token, linear_token):
        self.jira = JiraClient(jira_token)
        self.linear = LinearClient(linear_token)

    def collect_feature_metrics(self):
        features = self.linear.get_issues(
            filter={'state': {'name': {'in': ['In Progress', 'Done']}}},
            include=['state_changes', 'comments', 'assignee']
        )

        metrics = []
        for feature in features:
            cycle_time = self.calculate_cycle_time(feature)
            touch_points = self.count_handoffs(feature)
            complexity_score = self.assess_complexity(feature)

            metrics.append({
                'feature_id': feature.id,
                'cycle_time_hours': cycle_time,
                'touch_points': touch_points,
                'complexity': complexity_score,
                'timestamp': datetime.now()
            })

        return metrics

    def detect_bottlenecks(self, metrics):
        # Identify stages with >24h delays
        bottlenecks = []
        for metric in metrics:
            if metric['cycle_time_hours'] > 24:
                bottlenecks.append({
                    'feature': metric['feature_id'],
                    'delay_hours': metric['cycle_time_hours'],
                    'stage': self.identify_stuck_stage(metric)
                })
        return bottlenecks
```

**Metrics Captured**:
- Feature cycle time from ideation to delivery
- Work-in-progress limits and queue lengths
- Estimation accuracy and velocity trends
- Blockers and dependency resolution time

### Phase 2: Advanced Analytics (Week 2-3)

#### 4. Grafana Performance Dashboard
**Purpose**: Real-time visualization and trend analysis
**Implementation**:
```yaml
# Docker Compose for monitoring stack
version: '3.8'
services:
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=secure_password
    volumes:
      - ./grafana/dashboards:/var/lib/grafana/dashboards
      - ./grafana/provisioning:/etc/grafana/provisioning

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'

  process-exporter:
    build: ./process-metrics-exporter
    environment:
      - GITHUB_TOKEN=${GITHUB_TOKEN}
      - SLACK_TOKEN=${SLACK_TOKEN}
      - LINEAR_TOKEN=${LINEAR_TOKEN}
```

**Dashboard Panels**:
- Real-time cycle time trends with forecasting
- Team velocity and capacity utilization
- Quality metrics and defect detection rates
- Resource allocation and bottleneck visualization

#### 5. Automated Alerting System
**Purpose**: Proactive notification of process degradation
**Implementation**:
```javascript
// Alert rules configuration
const alertRules = [
  {
    name: 'Code Review Backlog',
    condition: 'avg(review_queue_length) > 5',
    threshold: 5,
    duration: '15m',
    severity: 'warning',
    action: 'notify_team_lead',
    message: 'Code review queue has {{.Value}} pending reviews (threshold: 5)'
  },
  {
    name: 'Cycle Time Regression',
    condition: 'avg_over_time(cycle_time_days[7d]) > avg_over_time(cycle_time_days[30d]) * 1.3',
    threshold: 1.3,
    duration: '1h',
    severity: 'critical',
    action: 'escalate_to_management',
    message: 'Cycle time has increased 30% above monthly average'
  },
  {
    name: 'Quality Degradation',
    condition: 'avg(defect_rate) > 0.15',
    threshold: 0.15,
    duration: '30m',
    severity: 'warning',
    action: 'notify_qa_team',
    message: 'Defect rate {{.Value}} exceeds quality threshold (0.15)'
  }
];

// Alert notification system
class AlertManager {
  async evaluateRules() {
    for (const rule of alertRules) {
      const currentValue = await this.queryMetric(rule.condition);
      if (currentValue > rule.threshold) {
        await this.triggerAlert(rule, currentValue);
      }
    }
  }

  async triggerAlert(rule, value) {
    const alert = {
      rule_name: rule.name,
      severity: rule.severity,
      value: value,
      threshold: rule.threshold,
      message: rule.message.replace('{{.Value}}', value),
      timestamp: new Date()
    };

    // Send to appropriate channels
    if (rule.action === 'notify_team_lead') {
      await this.sendSlackAlert(alert, '#team-leads');
    } else if (rule.action === 'escalate_to_management') {
      await this.sendEmailAlert(alert, 'engineering-managers@company.com');
    }
  }
}
```

**Alert Categories**:
- **Critical**: Process failures, security issues, system downtime
- **Warning**: Threshold breaches, trend degradation, capacity issues
- **Info**: Milestone achievements, optimization opportunities

### Phase 3: Optimization Engine (Week 4)

#### 6. Predictive Analytics
**Purpose**: Forecast performance and recommend optimizations
**Implementation**:
```python
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

class ProcessOptimizationEngine:
    def __init__(self):
        self.models = {}
        self.features = [
            'team_size', 'complexity_score', 'dependencies_count',
            'reviewer_availability', 'testing_queue_length',
            'historical_cycle_time', 'day_of_week', 'sprint_week'
        ]

    def train_cycle_time_predictor(self, historical_data):
        """Train ML model to predict feature cycle times"""
        df = pd.DataFrame(historical_data)
        X = df[self.features]
        y = df['actual_cycle_time_hours']

        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X, y)

        # Validate model accuracy
        predictions = model.predict(X)
        mae = mean_absolute_error(y, predictions)

        self.models['cycle_time'] = model
        return {
            'model_accuracy': f'Â±{mae:.1f} hours MAE',
            'feature_importance': dict(zip(self.features, model.feature_importances_))
        }

    def predict_delivery_date(self, feature_specs):
        """Predict when a feature will be delivered"""
        if 'cycle_time' not in self.models:
            return None

        feature_vector = self.extract_features(feature_specs)
        predicted_hours = self.models['cycle_time'].predict([feature_vector])[0]

        # Account for working hours and holidays
        delivery_date = self.add_business_hours(datetime.now(), predicted_hours)
        confidence = self.calculate_prediction_confidence(feature_specs)

        return {
            'estimated_delivery': delivery_date,
            'confidence_interval': f'Â±{confidence:.1f} days',
            'predicted_cycle_time': f'{predicted_hours:.1f} hours'
        }

    def identify_optimization_opportunities(self):
        """Analyze patterns and suggest improvements"""
        bottleneck_analysis = self.analyze_bottlenecks()
        resource_optimization = self.analyze_resource_allocation()
        quality_improvements = self.analyze_quality_patterns()

        return {
            'bottlenecks': bottleneck_analysis,
            'resource_optimization': resource_optimization,
            'quality_improvements': quality_improvements,
            'roi_ranking': self.rank_by_roi()
        }
```

**Optimization Insights**:
- Cycle time prediction with 85% accuracy
- Bottleneck identification with root cause analysis
- Resource allocation recommendations
- Quality improvement suggestions with ROI estimates

## Stakeholder-Specific Dashboards

### Engineering Team Dashboard
**Focus**: Operational metrics and daily workflow optimization
**Key Metrics**:
- Active work distribution across team members
- Code review queue and assignment optimization
- Build/test failure rates and resolution times
- Technical debt accumulation and reduction progress

### Product Team Dashboard
**Focus**: Feature delivery and customer value metrics
**Key Metrics**:
- Feature velocity and delivery predictability
- Requirements clarity and change request frequency
- Customer feedback integration speed
- Market impact and adoption rates

### Executive Dashboard
**Focus**: Strategic performance and business impact
**Key Metrics**:
- Overall process efficiency and cost per feature
- Team productivity trends and capacity planning
- Quality metrics and customer satisfaction correlation
- Competitive delivery speed and market responsiveness

## Implementation Roadmap

### Week 1: Foundation Setup
- **Day 1-2**: Install monitoring infrastructure (Grafana, Prometheus)
- **Day 3-4**: Configure basic metric collection (GitHub, Slack, Linear)
- **Day 5**: Create initial dashboards and validate data flow

### Week 2: Enhanced Monitoring
- **Day 1-2**: Implement advanced analytics and trend detection
- **Day 3-4**: Configure automated alerting system
- **Day 5**: Train team on dashboard usage and interpretation

### Week 3: Optimization Engine
- **Day 1-3**: Deploy predictive analytics and ML models
- **Day 4-5**: Create optimization recommendation engine

### Week 4: Validation & Tuning
- **Day 1-2**: Validate alert accuracy and reduce false positives
- **Day 3-4**: Fine-tune prediction models with real data
- **Day 5**: Document system and create maintenance procedures

## Success Metrics & ROI

### Quantifiable Benefits
- **Process Visibility**: 100% real-time visibility into all workflows
- **Response Time**: <15 minutes average alert response time
- **Prediction Accuracy**: 85% accuracy for cycle time estimates
- **Optimization Impact**: 20% reduction in process waste within 90 days

### Cost-Benefit Analysis
- **Setup Cost**: $15,000 (infrastructure + 80 hours engineering time)
- **Annual Operating Cost**: $8,400 (tools + maintenance)
- **Annual Savings**: $45,000 (proactive issue resolution + optimization)
- **ROI**: 248% first year, 435% steady state

### Key Performance Indicators
- **Alert Accuracy**: >90% actionable alerts, <5% false positives
- **Dashboard Utilization**: >80% daily active usage by stakeholders
- **Process Improvement Rate**: >3 optimizations implemented per month
- **Team Satisfaction**: >4.0/5.0 rating for monitoring system usefulness

## Integration Points

### Existing Tools
- **GitHub**: Automated workflow metric collection
- **Slack**: Real-time notifications and team communication tracking
- **Jira/Linear**: Project management and issue lifecycle monitoring
- **CI/CD**: Build and deployment performance tracking

### Data Flow
- **Collection**: Automated agents collect metrics from integrated tools
- **Processing**: Real-time analysis and pattern detection
- **Storage**: Time-series database for historical trend analysis
- **Visualization**: Real-time dashboards with customizable views
- **Alerting**: Intelligent notifications with escalation procedures

## Maintenance & Evolution

### Regular Reviews
- **Weekly**: Alert accuracy review and threshold adjustment
- **Monthly**: Dashboard effectiveness and stakeholder feedback
- **Quarterly**: Predictive model retraining and accuracy assessment
- **Annually**: Complete system architecture review and upgrade planning

### Continuous Improvement
- **Metric Expansion**: Add new metrics based on process evolution
- **Alert Refinement**: Improve alert accuracy through machine learning
- **Dashboard Enhancement**: Evolve visualizations based on user feedback
- **Integration Growth**: Connect additional tools as stack evolves

**Focus on creating actionable insights that drive measurable process improvements while maintaining low maintenance overhead and high system reliability.**
```

4. **Monitoring Setup Modes**:

### Comprehensive Setup (Default)
- Full monitoring infrastructure with real-time dashboards
- Automated alerting system with intelligent escalation
- Predictive analytics and optimization recommendations
- Multi-stakeholder dashboard views

### Process-Specific (`--process [id]`)
- Targeted monitoring for specific workflow
- Customized metrics and thresholds
- Process-specific optimization recommendations
- Focused alerting and dashboard views

### Alert Configuration (`--alerts`)
- Automated alerting system setup
- Threshold configuration and escalation rules
- Integration with communication tools
- Alert accuracy monitoring and tuning

### Dashboard Creation (`--dashboard`)
- Executive-level performance dashboard
- Real-time process health indicators
- Trend analysis and forecasting
- Stakeholder-specific metric views

## Parameters:
- `--process ID` - Set up monitoring for specific workflow
- `--alerts` - Configure automated alerting system
- `--dashboard` - Create executive performance dashboard
- `--metrics TYPES` - Specify metric categories (cycle-time, quality, throughput)
- `--tools STACK` - Integrate with specific tool stack
- `--stakeholder ROLE` - Customize dashboard for role (dev, product, exec)

## Monitoring Framework:

### Core Metrics Categories
- **Efficiency**: Cycle time, throughput, resource utilization, waste reduction
- **Quality**: Defect rates, first-pass success, customer satisfaction, rework frequency
- **Velocity**: Feature delivery rate, story point completion, sprint velocity, capacity utilization
- **Health**: Process stability, tool performance, team satisfaction, system reliability

### Alert Severity Levels
- **Critical**: Process failures, security breaches, system outages requiring immediate action
- **Warning**: Threshold breaches, performance degradation, capacity approaching limits
- **Info**: Milestone achievements, optimization opportunities, trend notifications

### Dashboard Types
- **Operational**: Real-time status, active work, immediate bottlenecks, daily metrics
- **Tactical**: Weekly trends, sprint performance, team velocity, quality metrics
- **Strategic**: Monthly performance, quarterly trends, capacity planning, ROI tracking

## Integration Points:
- **Input**: Workflow definitions, tool APIs, team metrics, performance baselines
- **Output**: Real-time dashboards, automated alerts, optimization recommendations
- **Monitoring**: System health tracking, alert accuracy, dashboard utilization
- **Optimization**: Continuous improvement based on monitoring insights

## Error Handling:
- Missing tool integrations: Provide setup guides and alternative data collection methods
- Insufficient historical data: Use industry benchmarks and gradual baseline establishment
- Alert fatigue: Implement intelligent thresholds and alert correlation
- Dashboard complexity: Provide role-based views and progressive disclosure

Focus on creating actionable monitoring systems that provide immediate value while building toward comprehensive process optimization and predictive analytics capabilities.